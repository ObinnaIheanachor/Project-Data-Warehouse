# Project Data Warehouse (AWS Redshift)
## Project Overview

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this project, we will build an ETL pipeline that extracts data from S3, stages them in Redshift, and transforms this data into a set of dimensional and fact tables for their analytics team to continue finding insights into what songs their users are listening to

## Song Dataset 
We will work with two datasets that reside in S3. 

#### Song Dataset: 
The **Songs dataset** is a subset of [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

Sample Data:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Dataset
The second dataset consists of log files in JSON format generated by this  [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset are partitioned by year and month. 

Sample Data: 

    {"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}


## Schema for Song Play Analysis

#### Fact Table
songplays - records in event data associated with song plays. Columns for the table:

    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables 
##### users

    user_id, first_name, last_name, gender, level
##### songs

    song_id, title, artist_id, year, duration

##### artists

    artist_id, name, location, lattitude, longitude

##### time

    start_time, hour, day, week, month, year, weekday
    
An entity relationship diagram (ERD) of the data model is given below;
![](./Images/erd.png)

## Project structure

Files in this repository:

|     File / Folder      |                         Description                          |
| :--------------------: | :----------------------------------------------------------: |
|         images         |  Contains images  |
| aws_cluster_create.py  | Creates a Redshift cluster on AWS with correct configurations |
| aws_cluster_delete.py |       Deletes the Redshift cluster on AWS        |
|     sql_queries.py     | Contains SQL queries for staging, schema definition and ETL |
|    create_tables.py    | Drops and creates tables on AWS Redshift (Reset the tables)  |
|         etl.py         | Stages and transforms the data from S3 buckets and loads them into tables |
|       count_queries.py       | Validate table creation steps and ensures tables are correctly created |
|        dwh.cfg         |              Sample configuration file for AWS               |
|         README         |                         Readme file                          |

<!-- GETTING STARTED -->

## Getting Started

Clone the repository into a local machine using

```sh
git clone https://github.com/ObinnaIheanachor/Project-Data-Warehouse
```

### Prerequisites

These are the prerequisites to run the program.

* > python 3.7
* PostgreSQL
* AWS account
* psycopg2 python library
* boto3 python library

### Steps to execute project
Follow these steps to extract and load the data into the data model.

1. Edit the `dwh.cfg` configuration file and fill in the AWS Access Key and Secret Key fields

2. Run `aws_cluster_create.py` to create the clusters on AWS by

   ```python
   python3 aws_cluster_create.py
   ```

   The type, number of nodes and other specifications of cluster will be created based on the config file. Ensure cluster creation confirmation message is displayed.

3. Run `create_tables.py` to create/reset the tables;

   ```python
   python3 create_tables.py
   ```

4. Run ETL script and load data into database; 

   ```python
   python3 etl.py
   ```

   This will run the SQL queries corresponding to staging data from S3 on Redshift, transforming, and inserting transformed data into the Postgres tables on Redshift.

5. Run `count_query.py` to validate the loading of data into tables;

   ```python
   python3 count_query.py
   ```

    This counts rows of data in all tables.

6. Run `aws_cluster_delete.py` to delete the cluster;

   ```python
   python3 aws_cluster_delete.py
   ```

